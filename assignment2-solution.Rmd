---
title: "Solution for MEM Assignment r2"
date: "`r Sys.Date()`"
author:
  - 胡浛
documentclass: ctexart
geometry: margin=1.18in # 页面边距
fontsize: 12pt
linestretch: 1.5 # 行间距
keywords:
  - assignment
output:
  rticles::ctex:
    fig_caption: yes # 显示图表的标题
    number_sections: no
    citation_package: natbib
    toc: no
---

```{r setup,include = FALSE}
knitr::opts_chunk$set(echo = FALSE,error = FALSE, warning = FALSE, message = FALSE,
                      out.width = "100%", split = FALSE, fig.align = "center")
#load library
library(dplyr)
library(tidyverse)
library(lubridate)
library(e1071)
library(showtext)
library(kableExtra)
library(gridExtra)
library(ggplot2)
library(readxl)

theme_set(theme(text = element_text(family="sans",size = 10)))
# 添加中文字体
font_add("PingFang", "/System/Library/Fonts/PingFang.ttc")  # 根据字体路径调整
showtext_auto()
```

## Question #1: BigBangTheory. (Attached Data: BigBangTheory)

```{r}
# 读取 CSV 文件并转换日期
bigbang_data <- read_csv(
  file = "./data/BigBangTheory.csv",
  skip = 1,
  col_names = c("air_date", "viewers")
) %>% 
  mutate(air_date = mdy(air_date))
```

```{r }
cat("a. Minimum Viewers:", min(bigbang_data$viewers), 
    ", Maximum Viewers:", max(bigbang_data$viewers), "\n")

cat("b. Mean =", mean(bigbang_data$viewers), 
    ", Median =", median(bigbang_data$viewers), 
    ", Mode =", names(which.max(table(bigbang_data$viewers))), "\n")

cat("c. Q1 =", quantile(bigbang_data$viewers, 0.25),
    ", Q2 = ", quantile(bigbang_data$viewers, 0.75), "\n")


bigbang_model <- lm(bigbang_data$viewers ~ bigbang_data$air_date)
bigbang_model_summary <- summary(bigbang_model)
p_value <- bigbang_model_summary$coefficients[2,4]

cat("d. p值>0.05, 回归系数不显著，从给出的数据没有看出有明显的上升或下降趋势")

```

```{r scatterplot, fig.cap= "viewer观众数变化趋势图", fig.width=8, fig.height=3, dpi=150}

ggplot(bigbang_data,aes(air_date,viewers)) +
         geom_point() +
         geom_line(color="blue") +
  scale_x_date(breaks = bigbang_data$air_date) +
  theme(axis.text.x = element_text(angle = 90), # 将 x 轴刻度标签旋转 90 度
        text = element_text(family = "PingFang")) 

```


## Question #2: NBAPlayerPts. (Attached Data: NBAPlayerPts)

```{r}
nba_players <- read_csv(file = "./data/NBAPlayerPts.csv", col_names = TRUE)
```

a. Show the frequency distribution.

ppg频率直方分布图如下图

```{r fig.width=8, fig.height=4, dpi=150}
ppg <- nba_players$PPG
#range(nba_players$PPG)
breaks <- seq(10, 30, by = 2)
# 分组 PPG
ppg_groups <- cut(ppg, breaks, right = FALSE)
# 计算频率分布
frequency <- table(ppg_groups)
print(frequency)
```

b. Show the relative frequency distribution.

```{r}

# 计算相对频率
relative_frequency <- prop.table(frequency)
kable(relative_frequency, 
      col.names = c("ppg分组", "相对频率(%)"), # 设置列名
      caption = "ppg相对频率分布表")

```

c. Show the cumulative percent frequency distribution.

```{r}

# 计算累计频率
cumulative_percent_frequency <- cumsum(relative_frequency) * 100
kable(cumulative_percent_frequency, 
      col.names = c("ppg分组", "累计频率(%)"),
      caption = "ppg累计频率分布表")

```

d. Develop a histogram for the average number of points scored per game.

```{r}

# 绘制直方图
hist(ppg, breaks = breaks, right = FALSE, 
     col = "skyblue", main = "Histogram of Average Points Per Game",
     xlab = "Points Per Game (PPG)", ylab = "Frequency")

```
e. Do the data appear to be skewed? Explain.

Skewness=`r skewness(ppg)` > 0, 数据右偏

f. What percentage of the players averaged at least 20 points per game?

场均得分至少为 20 分的球员占比: `r sum(ppg >= 20) / length(ppg) * 100`%.


## Question #3:

a. How large was the sample used in this survey?
b. What is the probability that the point estimate was within ±25 of the population mean?


```{r}

se <- 20        # 标准误差
sigma <- 500    # 总体标准差

# 计算样本量
n <- (sigma / se)^2
cat("样本量为:", n, "\n")

margin <- 25
# 计算 z 值
z <- margin / se
# 计算概率
probability <- pnorm(z) - pnorm(-z)
cat("样本均值在总体均值 ±25 范围内的概率为:", probability, "\n")

```


## Question #4: Young Professional Magazine (Attached Data: Professional)

```{r}
# 读取 CSV 文件
prof_data <- read_csv("./data/Professional.csv")
#str(prof_data)
# 去除空列、重命名列
prof_cleaned <- 
  prof_data[, !grepl("^\\.{3}", names(prof_data))] %>% 
  rename(
    age="Age",
    gender = "Gender",
    real_estate_purchases = "Real Estate Purchases?",
    investments = "Value of Investments ($)",
    transactions_count="Number of Transactions",
    access_broadband = "Broadband Access?",
    income = "Household Income ($)",
    have_childred = "Have Children?"
  )
# 转换列为因子
cols_to_factor <- c("gender", "real_estate_purchases", "access_broadband", "have_childred")
prof_cleaned[cols_to_factor] <- lapply(prof_cleaned[cols_to_factor], factor)
#str(prof_cleaned)
```

a. Develop appropriate descriptive statistics to summarize the data.

```{r}
summary(prof_cleaned)
```



```{r}
# 定义计算95%置信区间函数
z_95 <- qnorm(0.975, lower.tail = TRUE)
fn_z95ci <-function(x) {
  m <- mean(x, na.rm = TRUE)
  n <- sum(!is.na(x))
  s <- sd(x, na.rm = TRUE) / sqrt(n)
  c(m - z_95 * s, m + z_95 * s)
}
```

b. Develop 95% confidence intervals for the mean age and household income of subscribers.

平均年龄95%置信区间为 [`r fn_z95ci(prof_cleaned$age)`], 平均年收入95%的置信区间为 [`r fn_z95ci(prof_cleaned$income)`]。


```{r}
# 定义计算比率的置信区间函数
fn_z95ci_prop <-function(x,val) {
  n <- sum(!is.na(x))  # 有效样本量
  p <- sum(!is.na(x) & x == val) / n # 样本比率
  s <- sqrt(p*(1-p)/n)
  c(p - z_95 * s, p + z_95 * s)
}
```


c. Develop 95% confidence intervals for the proportion of subscribers who have broadband access at home and the proportion of subscribers who have children.

95% confidence intervals for the proportion of subscribers who have broadband access at home is [`r  fn_z95ci_prop(prof_cleaned$access_broadband, val = "Yes")`]; 

95% confidence intervals for the proportion of subscribers who have children is [`r fn_z95ci_prop(prof_cleaned$have_childred, val = "Yes")`]

d. Would *Young Professional* be a good advertising outlet for online brokers? Justify your conclusion with statistical data.

Yes. 如下图所示，按金融投资金额占家庭年收入的比值分为5组

1)占总人数80%以上的人Young Professional，金融投资占年收入比值>20%，这部分人群都是潜在的广告客户

2)接近62.4%的人群允许了广告投放

3)每组的平均金融交易次数没有因投资比例而呈现明显的差异，可能是因为缺少专业的 brokers

```{r}
prof_invest <- select(prof_cleaned, investments, transactions_count, income) %>% 
  mutate(
    investment_to_income_ratio = ifelse(
      !is.na(investments) & !is.na(income),
      investments / income * 100,
      NA
    ),
    investment_to_income_ratio_group = cut(
      investment_to_income_ratio,
      breaks = c(0, 20, 40, 60, 80, Inf), # 分为5组
      labels = c("0-20", "20-40", "40-60", "60-80", ">80"), # 设置组名
      include.lowest = TRUE # 包含最小值在第一组
    )
  )

prof_invest <- prof_invest %>% 
  group_by(investment_to_income_ratio_group) %>% 
  summarise(
    mean_transactions = mean(transactions_count),
    count = n()
  ) %>% 
  mutate(
    proportion = count / sum(count)
  )

# 使用 ggplot 绘制 Investment_Ratio_Group 与 平均交易次数 的条形图
plot1 <- ggplot(prof_invest, aes(x = investment_to_income_ratio_group, y = mean_transactions)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Investment Ratio Group",
       y = "Average Number of Transactions") +
  theme_minimal()

# 绘制条形图显示各分组的比例
plot2 <- ggplot(prof_invest, aes(x = investment_to_income_ratio_group, y = proportion)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(x = "Investment Ratio Group",
       y = "Proportion") +
  scale_y_continuous(labels = scales::percent) +  # 将 y 轴标签显示为百分比
  theme_minimal()


grid.arrange(plot1, plot2, ncol = 2)
```

e. Would this magazine be a good place to advertise for companies selling educational software and computer games for young children?

Yes. 有小孩的人群占比为53.4%。

```{r}

prof_cleaned %>% 
  group_by(have_childred) %>% 
  summarise(
    count = n(),
  ) %>% 
  mutate(
    prop = count / sum(count) * 100
  )

```

f. Comment on the types of articles you believe would be of interest to readers of *Young Professional*.

感兴趣的文章类型

1) 财经, 人群中投资比、收入均比较高
2) 育儿，有小孩的人群超过总人数的50%
3) 房产，有房产交易的占比44%
4) 运动，人群中青年人居多

```{r}

prof_cleaned %>% 
  group_by(real_estate_purchases) %>% 
  summarise(
    count = n(),
  ) %>% 
  mutate(
    prop = count / sum(count) * 100
  )

```


## Question #5: Quality Associate, Inc. (Attached Data: Quality)

a. Conduct a hypothesis test for each sample at the .01 level of significance and determine what action, if any, should be taken. Provide the p-value for each test.

the p-value for each test as below

```{r}

quality_data <- read.csv("./data/Quality.csv")

#str(quality_data)
#View(quality_data)

cal_p <- function(x, miu, sigma){
  m <- mean(x)
  n <- length(x)
  z_stat <- (m - miu) / (sigma/sqrt(n))
  p_value <- 2 * (1 - pnorm(abs(z_stat)))
  return(p_value)
}

#quality_data %>% 
#  map_dbl(cal_p, miu = 12, sigma = 0.21)
alpha <- 0.01
for (col in names(quality_data)) {
  p_value <- cal_p(quality_data[[col]], miu = 12, sigma = 0.21)
  action <- ifelse(p_value < alpha, "需要整改", "不需要整改")
  cat(col, "p值为", p_value, action, "\n")
}

```

b. compute the standard deviation for each of the four samples. does the assumption of .21 for the population standard deviation appear reasonable?

每个样本的标准差与预估标准差0.21的偏差均小于5%, 可以认为总体的标准差为0.21是比较合理的

```{r}
cal_deviation <- function(x, es_sd){
  s <- sd(x)
  abs(s- es_sd) / es_sd * 100
}

# 计算每列样本的标准差与0.21的偏差
std_devs <- sapply(quality_data, 
                          function(col) cal_deviation(col, es_sd = 0.21))
print(std_devs)

```

```{r}
# 定义函数计算控制限
calculate_control_limits <- function(alpha, mu, sigma, n) {
  # 参数说明：
  # alpha: 显著性水平（如 0.01 或 0.05）
  # mu: 总体均值
  # sigma: 总体标准差
  # n: 样本大小
  
  # 计算标准误差
  se <- sigma / sqrt(n)
  
  # 查找 z_alpha/2 值
  z_alpha <- qnorm(1 - alpha / 2)
  
  # 计算控制限
  upper_control_limit <- mu + z_alpha * se
  lower_control_limit <- mu - z_alpha * se
  
  # 返回结果为一个列表
  return(c(lower_control_limit, upper_control_limit))
}

calculate_control_limits(alpha = 0.01, mu = 12, sigma = 0.21, n = 30)

```

c. compute limits for the sample mean $\overline x$ around $\mu=12$ such that, as long as a new sample mean is within those limits, the process will be considered to be operating satisfactorily. if $\overline x$ exceeds the upper limit or if $\overline x$ is below the lower limit, corrective action will be taken. these limits are referred to as upper and lower control limits for quality control purposes.

lower and upper control limits is `r calculate_control_limits(alpha = 0.01, mu = 12, sigma = 0.21, n = 30)`

d. discuss the implications of changing the level of significance to a larger value. what mistake or error could increase if the level of significance is increased?

显著性水平增加，允许拒绝原假设的证据要求越低，测试更倾向于拒绝 $H_0$。
$\alpha$ =0.01调整为$\alpha$=0.05时，对假设检验的要求更宽松,第一类错误增加。

```{r}
calculate_control_limits(alpha = 0.01, mu = 12, sigma = 0.21, n = 30)
calculate_control_limits(alpha = 0.05, mu = 12, sigma = 0.21, n = 30)
```

## Question #6

a. Estimate the proportion of units rented during the first week of March 2007 and the first week of March 2008.

```{r}
occupancy_data <- read.csv("./data/Occupancy.csv", skip = 1)
# 转换为因子
occupancy_data[] <- lapply(occupancy_data, function(col) {
  factor(col, levels = c("Yes", "No"))
})

calc_occupancy_prop <- function(data){
  # 计算每列Yes的比例
  yes_proportions <- sapply(data, function(col) {
    sum(col == "Yes", na.rm = TRUE) / sum(!is.na(col))
  })
  return(yes_proportions)
}

yes_proportions <- calc_occupancy_prop(occupancy_data)
print(yes_proportions)
```

b. Provide a 95% confidence interval for the difference in proportions.

```{r}

p1 <- yes_proportions[1]
p2 <- yes_proportions[2]

n1 <- sum(!is.na(occupancy_data$March.2007))
n2 <- sum(!is.na(occupancy_data$March.2008))

# 差异置信区间计算
p_diff <- p2 - p1
se <- sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))
z <- 1.96  # 95%置信区间
ci_lower <- p_diff - z * se
ci_upper <- p_diff + z * se

cat("出租比例差异的95%置信区间: [", ci_lower, ",", ci_upper, "]\n")

```

c. On the basis of your findings, does it appear March rental rates for 2008 will be up from those a year earlier?

是的。置信区间的下限为正数，说明2008年出租率显著高于2007年。因为在该置信区间内，所有可能的差异值都表明2008年出租率高于2007年

## Question #7: Air Force Training Program (data file: Training)

a. use appropriate descriptive statistics to summarize the training time data for each method. what similarities or differences do you observe from the sample data?

```{r}

training_data <- read.csv("./data/Training.csv")
#str(training_data)

skimr::skim(training_data) %>% 
  kable() %>% 
  kable_styling(position = "center", bootstrap_options = c("striped", "hover"), latex_options = c("scale_down"), font_size = 6)

```

b. Comment on any difference between the population means for the two methods. Discuss your findings.

```{r}

current <- training_data$Current
proposed <- training_data$Proposed

mean(current)
mean(proposed)

# t检验比较两种训练方法的均值
t_test_result <- t.test(current, proposed)
# p-value=0.55,大于0.05,说明current和proposed的总体均值之间没有显著差异
t_test_result
```

c. compute the standard deviation and variance for each training method. conduct a hypothesis test about the equality of population variances for the two training methods. Discuss your findings.

```{r}

# 输出方差和标准差
cat("Method 1 - Variance:", var(current), "SD:", sd(current), "\n")
cat("Method 2 - Variance:", var(proposed), "SD:", sd(proposed), "\n")

# F检验比较方差
f_test_result <- var.test(current, proposed)
f_test_result

```

Current和Proposed的方差存在显著差异，Proposed的方差更小。

d. what conclusion can you reach about any differences between the two methods? what is your recommendation? explain.

学习通过两种方式培训的时长均值不存在显著差异，但Proposed的方差更小，说明时长分布相对密集，为了减少其他同学的等待时长，选择Proposed的方式更佳。


e. can you suggest other data or testing that might be desirable before making a final decision on the training program to be used in the future?

为了做出全面的决策，除了培训时长外，还需要考虑培训后的效果，完成培训后，记录每个学生在培训后的表现，分析两组的培训效果是否存在差异。


## Q8

a. Develop a scatter diagram with the car mileage on the horizontal axis and the price on the vertical axis.

```{r}

camry_data <- read.csv("./data/Camry.csv", col.names = c("Mileage", "Price"))
#str(camry_data)
#View(camry_data)

# 使用 ggplot2 绘制散点图
ggplot(camry_data, aes(x = Mileage, y = Price)) +
  geom_point(color = "blue") +  # 设置点的颜色
  labs(title = "2007 Toyota Camry Mileage vs Price",
       x = "Mileage (Thousands of miles)",
       y = "Price (Thousands of dollars)") +
  theme_minimal()  # 使用简洁的主题
```

b. what does the scatter diagram developed in part (a) indicate about the relationship between the two variables?

曲线整体是一个向下倾斜的趋势，说明随着里程的增加，价格可能在下降。

c. Develop the estimated regression equation that could be used to predict the price ($1000s) given the miles (1000s).

```{r}

# 执行线性回归
model <- lm(Price ~ Mileage, data = camry_data)

# 查看回归模型的结果
summary(model)

# 提取回归方程的截距和斜率
intercept <- coef(model)[1]
slope <- coef(model)[2]

# 输出回归方程
cat("回归方程为：Price = ", intercept, " + ", slope, " * Mileage\n")

# 使用回归方程进行预测（假设给定里程为60,000英里，即60）
predicted_price <- predict(model, newdata = data.frame(Mileage = 60))

# 输出预测价格
cat("预测里程为60,000英里的价格为：", predicted_price, "千美元\n")

equation <- paste("Price = ", round(intercept, 2), " + ", round(slope, 2), " * Mileage")

# 创建 ggplot 图形：绘制散点图和回归拟合线，并添加回归方程
ggplot(camry_data, aes(x = Mileage, y = Price)) +
  geom_point(color = "blue", size = 3) +  # 散点图，点的颜色为蓝色，大小为3
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "solid", size = 1) +  # 添加回归拟合曲线（红色实线）
  labs(title = "2007 Toyota Camry Mileage vs Price with Fitted Line",  # 标题
       x = "Mileage (Thousands of miles)",  # x轴标签
       y = "Price (Thousands of dollars)") +  # y轴标签
  annotate("text", x = 40, y = 15, label = equation, color = "black", size = 2, hjust = 0) +  # 添加回归方程式
  theme_minimal()  # 使用简洁的主题

```

d. Test for a significant relationship at the .05 level of significance.

斜率（Mileage）的 p 值为 0.0003，远小于0.05，说明我们拒绝零假设，即里程与价格之间存在显著的负相关关系

```{r}

# 执行线性回归分析
model <- lm(Price ~ Mileage, data = camry_data)

# 输出回归分析的结果
summary(model)

```

e. Did the estimated regression equation provide a good fit? Explain.

```{r}

# 计算 R-squared 值
r_squared <- summary(model)$r.squared
cat("R-squared:", r_squared, "\n")

# 计算 F 统计量和 p 值
# 如果 F 统计量的 p 值小于 0.05，表示回归模型显著，能够有效地解释数据的变异。

f_statistic <- summary(model)$fstatistic[1]
f_p_value <- pf(f_statistic, df1 = 1, df2 = nrow(camry_data) - 2, lower.tail = FALSE)
cat("F-statistic:", f_statistic, "\n")
cat("F p-value:", f_p_value, "\n")

# 绘制残差图
# 如果残差图中的点分布没有明显的模式，说明模型的拟合较好。
plot(model$residuals, main = "Residuals Plot", xlab = "Index", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)  # 添加水平线，表示零残差

```

f. Provide an interpretation for the slope of the estimated regression equation.

这意味着每行驶1000英里，汽车的价值下降大约60美元。

g. Suppose that you are considering purchasing a previously owned 2007 Camry that has been driven 60,000 miles. Using the estimated regression equation developed in part (c), predict the price for this car. Is this the price you would offer the seller.

使用回归方程预测价格为12.94千美元, 考虑当前二手车市场低迷，可以在这个价格上适当压价。

## Q9


```{r}
# 读取数据
we_data <- read_excel("./data/WE.xlsx") %>% 
  rename(
    customer_id = "客户ID",
    loss_flag = "流失",
    happiness_degree = "当月客户幸福指数",
    happiness_degree_change = "客户幸福指数相比上月变化",
    support = "当月客户支持",
    support_change = "客户支持相比上月的变化",
    service_priority = "当月服务优先级",
    service_priority_change = "服务优先级相比上月的变化",
    login_times = "当月登录次数",
    blogs_change = "博客数相比上月的变化",
    visits_change = "访问次数相比上月的增加",
    use_time = "客户使用期限",
    visit_interval = "访问间隔变化" # >0访问频率降低，<0访问频率增加
  ) %>% 
  mutate_at(vars(loss_flag), as.factor)
```


a. 通过可视化探索流失客户与非流失客户的行为特点（或特点对比），你能发现流失与非流失客户行为在哪些指标有可能存在显著不同？

1. 相对于非流失客户，流失客户当月幸福指数可能更低
2. 相对于非流失客户，流失客户访问间隔更大，流失客户的访问频率可能更低
3. 相对于非流失客户，流失客户当月支持、当月服务优先级可能更低；当月登录次数、博客数相比上月的变化、访问次数都相对较低


```{r}
# 绘制幸福指数箱线图
ggplot(we_data, aes(x = loss_flag, y = happiness_degree, fill = loss_flag)) +
  geom_boxplot() +
  theme_minimal() +
  labs(
    title = "幸福指数在不同流失状态下的分布",
    x = "流失状态",
    y = "幸福指数"
  )

# 绘制访问间隔变化箱线图
ggplot(we_data, aes(x = loss_flag, y = visit_interval, fill = loss_flag)) +
  geom_boxplot() +
  theme_minimal() +
  labs(
    title = "访问间隔变化在不同流失状态下的分布",
    x = "流失状态",
    y = "访问间隔变化"
  )
```





```{r fig.width=6, fig.height=4, dpi=150}

we_data %>% 
  group_by(loss_flag) %>% 
  summarise(
    m_support = mean(support),
    m_service_priority = mean(service_priority),
    m_login_times = mean(login_times),
    m_visits_change = mean(visits_change),
    m_blogs_change = mean(blogs_change)
  ) %>% 
  kable(caption = "流失与非流失客户关键指标均值对比") %>% 
  kable_styling(latex_options = c("scale_down"), font_size = 8)  # 缩放表格
```


b. 通过均值比较的方式验证上述不同是否显著。


```{r}

# 选择数值型变量进行检验
numeric_vars <- we_data %>% 
  select(-customer_id, -loss_flag) %>% 
  names()

# 定义检验函数
test_results <- numeric_vars %>%
  purrr::map_dfr(~ { #map_dfr对每个变量名生成一个数据框（行），最后将所有结果行绑定为一个数据框
    var_name <- .x #.x 是 map 遍历的当前变量名（字符串）
    
    # 按 loss_flag 分组计算均值
    loss_mean <- we_data %>%
      filter(loss_flag == 1) %>%
      summarise(mean_val = mean(.data[[var_name]], na.rm = TRUE)) %>%
      pull(mean_val)
    
    no_loss_mean <- we_data %>%
      filter(loss_flag == 0) %>%
      summarise(mean_val = mean(.data[[var_name]], na.rm = TRUE)) %>%
      pull(mean_val)
    
    test <- t.test(
      we_data[[var_name]] ~ we_data$loss_flag,# 指定待比较的变量和分组变量
      var.equal = FALSE  # 默认使用 Welch t 检验，不假定两组方差相等
    )
    
    tibble(
      Variable = var_name,
      `Mean (Loss)` = loss_mean,
      `Mean (No Loss)` = no_loss_mean,
      p_value = test$p.value
    )
  })

# 添加显著性标记
test_results <- test_results %>%
  mutate(Significance = case_when(
    p_value < 0.001 ~ "***",
    p_value < 0.01 ~ "**",
    p_value < 0.05 ~ "*",
    TRUE ~ "NS"
  ))

# 输出表格
test_results %>% 
  kable(caption = "流失与非流失客户关键指标显著性对比") %>% 
  kable_styling()
```
从上表可以看出，除了support_change、service_priority_change、visits_change其他变量都可能是影响客户流失的显著性性指标。

c. 以”流失“为因变量，其他你认为重要的变量为自变量（提示：a、b两步的发现），建立回归方程对是否流失进行预测。



```{r}

# 建立逻辑回归模型
we_model <- glm(loss_flag ~ happiness_degree + happiness_degree_change + support + service_priority + login_times + blogs_change + use_time + visit_interval, data = we_data, family = binomial)

# 查看模型结果
summary(we_model)
```

按上述模型的结果，对是否流失的显著变量有

1. 当月客户幸福指数(happiness_degree)及其相比上月变化(happiness_degree_change)与流失概率负相关, 用户当月幸福度的提升会降低流失的概率
2. 客户使用期限(use_time)越大，用户流失的概率越高，表明老用户更易流失
3. 用户访问间隔(visit_interval)越大，用户流失的概率越高，用户越不活跃越易流失


d. 根据上一步预测的结果，对尚未流失（流失=0）的客户进行流失可能性排序，并给出流失可能性最大的前100名用户ID列表。

流失可能性最大的前100名用户如下表，针对这些高风险客户，可采取以下措施: 

1. 设计促活措施，缩短客户访问间隔，比如定期推送内容、发放优惠券；
2. 持续为老客户提供价值，避免倦怠感，提供老用户的增值服务

```{r}
we_data %>% 
  mutate(
    predicted_prob = predict(we_model, type = "response")  # 计算流失概率
  ) %>% 
  filter(loss_flag == 0) %>% 
  arrange(desc(predicted_prob)) %>% 
  select(customer_id, predicted_prob) %>% 
  slice_head(n = 10) %>% # 表格太长，这里只打印出前10
  kable(caption = "流失可能性最大的前10名用户ID列表") %>% 
  kable_styling()
```


